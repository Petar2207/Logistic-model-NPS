{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tolUeZrAKeSh"
      },
      "outputs": [],
      "source": [
        "!pip install numpy\n",
        "!pip install pandas\n",
        "!pip install seaborn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pandas is a software library written for the Python programming language for data manipulation and analysis.\n",
        "import pandas as pd\n",
        "# NumPy is a library for the Python programming language, adding support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays\n",
        "import numpy as np\n",
        "# Matplotlib is a plotting library for python and pyplot gives us a MatLab like plotting framework. We will use this in our plotter function to plot data.\n",
        "import matplotlib.pyplot as plt\n",
        "#Seaborn is a Python data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics\n",
        "import seaborn as sns\n",
        "# Preprocessing allows us to standarsize our data\n",
        "from sklearn import preprocessing\n",
        "# Allows us to split our data into training and testing data\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Allows us to test parameters of classification algorithms and find the best one\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "# Logistic Regression classification algorithm\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "# Support Vector Machine classification algorithm\n",
        "from sklearn.svm import SVC\n",
        "# Decision Tree classification algorithm\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "# K Nearest Neighbors classification algorithm\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer"
      ],
      "metadata": {
        "id": "NdIPULlOLYjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix(y,y_predict):\n",
        "    \"this function plots the confusion matrix\"\n",
        "    from sklearn.metrics import confusion_matrix\n",
        "\n",
        "    cm = confusion_matrix(y, y_predict)\n",
        "    ax= plt.subplot()\n",
        "    sns.heatmap(cm, annot=True, ax = ax); #annot=True to annotate cells\n",
        "    ax.set_xlabel('Predicted labels')\n",
        "    ax.set_ylabel('True labels')\n",
        "    ax.set_title('Confusion Matrix');\n",
        "    ax.xaxis.set_ticklabels(['did not land', 'land']); ax.yaxis.set_ticklabels(['did not land', 'landed'])\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "TMTbPOVpLj1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload your CSV from local\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Load it into a DataFrame\n",
        "df = pd.read_csv(\"xy.csv\")  # replace with actual filename\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "iJJwugPcL9uE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ STEP 1: Define exclusions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "question_groups_to_exclude = ['']\n",
        "manually_excluded = [col for col in df.columns if any(col == q or col.startswith(f\"{q}A\") for q in question_groups_to_exclude)]\n",
        "missing_flags = [c for c in df.columns if '_missing' in c]\n",
        "excluded_columns = set(manually_excluded + missing_flags)\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ STEP 2: Identify columns to standardize â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Include columns with >2 unique values or known ordinal 2-value columns\n",
        "force_standardize = ['']  # Add more ordinal columns with 2 values as needed\n",
        "\n",
        "columns_to_standardize = [\n",
        "    c for c in df.columns\n",
        "    if c not in excluded_columns and (df[c].nunique() > 2 or c in force_standardize)\n",
        "]\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ STEP 3: Flip scale direction if needed â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "always_normal_direction = ['']  # Add known \"positive-direction\" columns here if needed\n",
        "\n",
        "df_proc = df.copy()\n",
        "cols_to_flip = []\n",
        "\n",
        "for c in columns_to_standardize:\n",
        "    if c in always_normal_direction:\n",
        "        continue\n",
        "    try:\n",
        "        df_proc[c] = pd.to_numeric(df_proc[c], errors='coerce')\n",
        "        col_min, col_max = df_proc[c].min(), df_proc[c].max()\n",
        "        df_proc[c] = col_max + col_min - df_proc[c]\n",
        "        cols_to_flip.append(c)\n",
        "    except:\n",
        "        print(f\"âš ï¸ Skipping flip: {c} contains non-numeric values\")\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ STEP 4: Ensure all columns are numeric â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "for col in columns_to_standardize:\n",
        "    df_proc[col] = pd.to_numeric(df_proc[col], errors='coerce')\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ STEP 5: Impute missing values â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "imputer = SimpleImputer(strategy=\"median\")\n",
        "df_proc[columns_to_standardize] = imputer.fit_transform(df_proc[columns_to_standardize])\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ STEP 6: Standardize features â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "scaler = StandardScaler()\n",
        "df_proc[columns_to_standardize] = scaler.fit_transform(df_proc[columns_to_standardize])\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ STEP 7: Prepare model inputs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "df_proc.to_csv(\"standardized_selected_columns.csv\", index=False)\n",
        "\n",
        "X = df.drop(columns=[''])              # original, for reference\n",
        "X_scaled = df_proc.drop(columns=[''])  # scaled version for modeling\n",
        "Y = df[''].astype(int).to_numpy()      # target column\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(\n",
        "    X_scaled, Y, test_size=0.2, random_state=2\n",
        ")"
      ],
      "metadata": {
        "id": "wPFCcyC9VL1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y = df[''].to_numpy()\n"
      ],
      "metadata": {
        "id": "qseaNK37NgHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_scaled = df_proc.drop(columns=[''])"
      ],
      "metadata": {
        "id": "HOeSnue5sN7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y, test_size=0.2, random_state=2)\n"
      ],
      "metadata": {
        "id": "D3z-TA5lONRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_test.shape\n"
      ],
      "metadata": {
        "id": "1B_lTpOuO7YM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parameters ={'C':[0.01,0.1,1],\n",
        "             'penalty':['l2'],\n",
        "             'solver':['lbfgs']}"
      ],
      "metadata": {
        "id": "r1Ju5kLKPSuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parameters ={\"C\":[0.01,0.1,1],'penalty':['l2'], 'solver':['lbfgs']}# l1 lasso l2 ridge\n",
        "lr=LogisticRegression()\n",
        "logreg_cv = GridSearchCV(estimator=lr, param_grid=parameters, cv=10)\n",
        "\n",
        "\n",
        "# Step 4: Fit the GridSearchCV object to the training data\n",
        "logreg_cv.fit(X_train, Y_train)"
      ],
      "metadata": {
        "id": "C40Ldf4iPXjX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nâ˜… Logistic Regression\")\n",
        "print(\"    best params  :\", logreg_cv.best_params_)\n",
        "print(f\"    CV accuracy  : {logreg_cv.best_score_:.3f}\")\n",
        "print(f\"    TEST accuracy: {logreg_cv.score(X_test, Y_test):.3f}\")\n",
        "\n",
        "# Predict on test set\n",
        "Y_pred = logreg_cv.predict(X_test)\n"
      ],
      "metadata": {
        "id": "AygVLpfbPaAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_lr = logreg_cv.best_estimator_\n",
        "coef = pd.Series(best_lr.coef_[0], index=X.columns, name=\"Î²\") \\\n",
        "          .sort_values(key=lambda s: s.abs(), ascending=False)\n",
        "\n",
        "display(coef.to_frame())        # full table\n",
        "\n",
        "(\n",
        "    coef.head(10)\n",
        "        .sort_values()          # order for tidy barh\n",
        "        .plot(kind=\"barh\", figsize=(7, 4))\n",
        ")\n",
        "plt.title(\"Top-10 Logistic-Regression feature weights\")\n",
        "plt.xlabel(\"Coefficient (Î²)\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CLJaVCWlREQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Get the best model from GridSearchCV\n",
        "best_lr = logreg_cv.best_estimator_\n",
        "\n",
        "# Get coefficients (Î² values)\n",
        "coefs = best_lr.coef_[0]  # shape: (n_features,)\n",
        "\n",
        "# Multiply each feature value by its corresponding coefficient\n",
        "# This gives the contribution of each feature per row\n",
        "contributions = X_scaled * coefs  # shape: (n_samples, n_features)\n",
        "\n",
        "# Take the mean of absolute contributions across all rows\n",
        "average_absolute_impact = np.abs(contributions).mean(axis=0)\n",
        "\n",
        "# Create a Pandas Series with feature names\n",
        "impact_series = pd.Series(average_absolute_impact, index=X.columns)\n",
        "\n",
        "# Sort by most impactful features\n",
        "impact_series = impact_series.sort_values(ascending=False)\n",
        "\n",
        "# Show results\n",
        "print(\"\\nğŸ” Average Absolute Contribution of Each Feature:\\n\")\n",
        "display(impact_series.to_frame(name=\"Average Impact\"))\n",
        "\n",
        "# Optional: plot the top 10 features\n",
        "impact_series.head(10).sort_values().plot(kind=\"barh\", figsize=(7, 4))\n",
        "plt.title(\"Top 10 Feature Impacts (Average Absolute Contribution)\")\n",
        "plt.xlabel(\"Average Contribution to Prediction\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "O7hndnnUmKE3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}