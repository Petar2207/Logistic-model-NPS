{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install shap"
      ],
      "metadata": {
        "id": "JRzOVK0bBEYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install openpyxl\n"
      ],
      "metadata": {
        "id": "5VyNXGhefzoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "from google.colab import files\n",
        "# Pandas is a software library written for the Python programming language for data manipulation and analysis.\n",
        "import pandas as pd\n",
        "# NumPy is a library for the Python programming language, adding support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays\n",
        "import numpy as np\n",
        "# Matplotlib is a plotting library for python and pyplot gives us a MatLab like plotting framework. We will use this in our plotter function to plot data.\n",
        "import matplotlib.pyplot as plt\n",
        "#Seaborn is a Python data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics\n",
        "import seaborn as sns\n",
        "# Preprocessing allows us to standarsize our data\n",
        "from sklearn import preprocessing\n",
        "# Allows us to split our data into training and testing data\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Allows us to test parameters of classification algorithms and find the best one\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "# Logistic Regression classification algorithm\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "# Support Vector Machine classification algorithm\n",
        "from sklearn.svm import SVC\n",
        "# Decision Tree classification algorithm\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "# K Nearest Neighbors classification algorithm\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer"
      ],
      "metadata": {
        "id": "NdIPULlOLYjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jWgMtiUWOWNb"
      },
      "outputs": [],
      "source": [
        "# Upload your CSV from local\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load it into a DataFrame\n",
        "df = pd.read_excel(\"combined_private.xlsx\")  # replace with actual filename"
      ],
      "metadata": {
        "id": "VbsSSoYttZqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load it into a DataFrame\n",
        "df_pitanja = pd.read_excel(\"combined_questions_private.xlsx\")  # replace with actual filename"
      ],
      "metadata": {
        "id": "6_UUwhXIPsc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(columns=[\n",
        "    'source_file'\n",
        "])\n"
      ],
      "metadata": {
        "id": "SQhiWpxiPLZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ordinal_vars = []\n",
        "\n",
        "for qid, qtype, options in zip(df_pitanja[\"QuestionID\"], df_pitanja[\"Type\"], df_pitanja[\"Options\"]):\n",
        "    if qtype == \"Scale\" and pd.notnull(options):\n",
        "        choices = [opt.strip() for opt in str(options).split(';') if '=' in opt]\n",
        "        if len(choices) > 3:   # more than 3 options\n",
        "            ordinal_vars.append(str(qid))\n",
        "\n",
        "# same cleanup you used (remove .0)\n",
        "ordinal_vars = [str(int(float(col))) if str(col).endswith('.0') else str(col) for col in ordinal_vars]"
      ],
      "metadata": {
        "id": "AvbwqaabLlDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "select_one = []\n",
        "for qid, qtype in zip(df_pitanja[\"QuestionID\"], df_pitanja[\"Type\"]):\n",
        "    if qtype == \"SelectOne\":\n",
        "        select_one.append(qid)\n",
        "select_one = [str(int(float(col))) if str(col).endswith('.0') else str(col) for col in select_one]"
      ],
      "metadata": {
        "id": "Q6GYDCe39Z9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "multi_nominal_cols = []\n",
        "for qid, qtype in zip(df_pitanja[\"QuestionID\"], df_pitanja[\"Type\"]):\n",
        "    if qtype == \"SelectMultiple\":\n",
        "        multi_nominal_cols.append(qid)\n",
        "multi_nominal_cols = [str(int(float(col))) if str(col).endswith('.0') else str(col) for col in multi_nominal_cols]"
      ],
      "metadata": {
        "id": "MHkQZ4oMORwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "binary = []\n",
        "\n",
        "for qid, qtype, options in zip(df_pitanja[\"QuestionID\"], df_pitanja[\"Type\"], df_pitanja[\"Options\"]):\n",
        "    if qtype == \"Scale\" and pd.notnull(options):\n",
        "        choices = [opt.strip() for opt in str(options).split(';') if '=' in opt]\n",
        "        if len(choices) <= 3:   # <= 3 options\n",
        "            binary.append(str(qid))\n",
        "\n",
        "binary = [str(int(float(col))) if str(col).endswith('.0') else str(col) for col in binary]"
      ],
      "metadata": {
        "id": "nBlk-heUI4vE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# See missing values per column\n",
        "df_mising=df.isnull().sum().sort_values(ascending=False)"
      ],
      "metadata": {
        "id": "fV6tVgjMOjcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_mising"
      ],
      "metadata": {
        "id": "HNsZWIO0dQo0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "za_drop = []\n",
        "\n",
        "for index, value in df_mising.items():\n",
        "    if value > 5000:\n",
        "        za_drop.append(index)"
      ],
      "metadata": {
        "id": "-uVmpoMBPZn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "za_drop = [str(int(float(col))) if str(col).endswith('.0') else str(col) for col in za_drop]"
      ],
      "metadata": {
        "id": "cTfaXl8FaUSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "za_drop2 = []\n",
        "\n",
        "for qid, qtype in zip(df_pitanja[\"QuestionID\"], df_pitanja[\"Type\"]):\n",
        "    if qtype == \"Text\":\n",
        "        za_drop2.append(str(qid))  # convert to string here\n"
      ],
      "metadata": {
        "id": "fLP7JyvAT_LT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "za_drop2_cleaned = [str(int(float(col))) if str(col).endswith('.0') else str(col) for col in za_drop2]"
      ],
      "metadata": {
        "id": "KqjqMGvhWio_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ğŸ”¹ Step 2: Drop rows where 379.1 is missing\n",
        "df = df[df['381'].notna()]"
      ],
      "metadata": {
        "id": "8e76VLOkGTJg",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(columns=za_drop)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "akacyr9eHLw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(columns=za_drop2_cleaned, errors='ignore')\n"
      ],
      "metadata": {
        "id": "sBfYaE0OUf7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Filter out columns that donâ€™t exist in the DataFrame\n",
        "ordinal_vars = [col for col in ordinal_vars if col in df.columns]\n",
        "\n",
        "# Step 3: Convert only valid ones\n",
        "for col in ordinal_vars:\n",
        "    df[col] = df[col].astype('Int64')\n"
      ],
      "metadata": {
        "id": "BNg8lB6TRLli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in multi_nominal_cols:\n",
        "    if col in df.columns:\n",
        "        # Step 1: Clean missing and convert to string\n",
        "        df[col] = df[col].fillna('').astype(str)\n",
        "\n",
        "        # Step 2: One-hot encode selections\n",
        "        dummies = df[col].str.get_dummies(sep=';')\n",
        "        dummies.columns = [f\"{col}_{c.strip()}\" for c in dummies.columns]\n",
        "\n",
        "        # Step 3: Create missing-all flag BEFORE dropping or joining\n",
        "        df[f'{col}_missing_all'] = dummies.sum(axis=1).eq(0).astype(int)\n",
        "\n",
        "        # Step 4: Replace original with dummies\n",
        "        df = df.drop(columns=[col])\n",
        "        df = pd.concat([df, dummies], axis=1)\n"
      ],
      "metadata": {
        "id": "wtdNwZCTRZYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in select_one:\n",
        "    if col in df.columns:\n",
        "        # Step 1: Clean missing and convert to string\n",
        "        df[col] = df[col].fillna('').astype(str)\n",
        "\n",
        "        # Step 2: One-hot encode selections\n",
        "        dummies = df[col].str.get_dummies(sep=',')\n",
        "        dummies.columns = [f\"{col}_{c.strip()}\" for c in dummies.columns]\n",
        "\n",
        "        # Step 3: Create missing-all flag BEFORE dropping or joining\n",
        "        df[f'{col}_missing_all'] = dummies.sum(axis=1).eq(0).astype(int)\n",
        "\n",
        "        # Step 4: Replace original with dummies\n",
        "        df = df.drop(columns=[col])\n",
        "        df = pd.concat([df, dummies], axis=1)\n"
      ],
      "metadata": {
        "id": "cI8Y_BprTCi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in binary:\n",
        "    if col in df.columns:\n",
        "        # Step 1: Clean missing and convert to string\n",
        "        df[col] = df[col].fillna('').astype(str)\n",
        "\n",
        "        # Step 2: One-hot encode selections\n",
        "        dummies = df[col].str.get_dummies(sep=',')\n",
        "        dummies.columns = [f\"{col}_{c.strip()}\" for c in dummies.columns]\n",
        "\n",
        "        # Step 3: Create missing-all flag BEFORE dropping or joining\n",
        "        df[f'{col}_missing_all'] = dummies.sum(axis=1).eq(0).astype(int)\n",
        "\n",
        "        # Step 4: Replace original with dummies\n",
        "        df = df.drop(columns=[col])\n",
        "        df = pd.concat([df, dummies], axis=1)"
      ],
      "metadata": {
        "id": "NQw3ajsIF7mq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Compute correlation matrix\n",
        "corr = df.corr()\n",
        "\n",
        "# Extract upper triangle (to avoid duplicates)\n",
        "upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
        "\n",
        "# Unstack into long form\n",
        "strong_corr = (\n",
        "    upper.stack()\n",
        "    .reset_index()\n",
        "    .rename(columns={0: \"Correlation\", \"level_0\": \"Var1\", \"level_1\": \"Var2\"})\n",
        ")\n",
        "\n",
        "# Filter for absolute correlation above threshold\n",
        "threshold = 0.8\n",
        "strong_corr = strong_corr[strong_corr[\"Correlation\"].abs() > threshold]\n",
        "\n",
        "print(strong_corr)"
      ],
      "metadata": {
        "id": "GbtJP20VMFvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def nps_reverse(score):\n",
        "    if score in [1, 2]:\n",
        "        return 1\n",
        "    elif score in [3, 4]:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "df[\"nps_category\"] = df[\"381\"].apply(nps_reverse)\n"
      ],
      "metadata": {
        "id": "BgW0gRgHwKGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cols_to_drop = [\"381\"]\n",
        "df = df.drop(columns=cols_to_drop)"
      ],
      "metadata": {
        "id": "FaUqNdPZ9Jit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ STEP 1: Identify target and excluded columns â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "target_col = \"nps_category\"\n",
        "excluded_columns = [target_col]  # add any other cols you donâ€™t want to touch\n",
        "\n",
        "# columns that are already in the \"normal/positive\" direction (DON'T flip these)\n",
        "always_normal_direction = [\"561\"]  # put your known positive-direction columns here\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ STEP 2: Determine which columns to standardize â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# (exclude target; only columns with >2 unique values; numeric-only for scaling)\n",
        "columns_to_standardize = [\n",
        "    c for c in df.columns\n",
        "    if c not in excluded_columns and df[c].nunique(dropna=True) > 2\n",
        "]\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ STEP 3: Flip scale direction if needed (skip binaries just in case) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "df_proc = df.copy()\n",
        "cols_to_flip = []\n",
        "\n",
        "for c in columns_to_standardize:\n",
        "    if c in always_normal_direction:\n",
        "        continue\n",
        "    if df[c].nunique(dropna=True) == 2:\n",
        "        continue  # safety: skip binary columns\n",
        "    try:\n",
        "        df_proc[c] = pd.to_numeric(df_proc[c], errors=\"coerce\")\n",
        "        col_min, col_max = df_proc[c].min(), df_proc[c].max()\n",
        "        df_proc[c] = col_max + col_min - df_proc[c]\n",
        "        cols_to_flip.append(c)\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Skipping flip: {c} ({e})\")\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ STEP 4: Ensure numeric columns â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "for c in columns_to_standardize:\n",
        "    df_proc[c] = pd.to_numeric(df_proc[c], errors=\"coerce\")\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ STEP 5: Prepare model inputs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "X = df_proc.drop(columns=[target_col])\n",
        "Y = df_proc[target_col]\n",
        "\n",
        "# Split FIRST âœ… (so imputer/scaler learn only from training data)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(\n",
        "    X, Y, test_size=0.2, random_state=2\n",
        ")\n",
        "\n",
        "# Only standardize columns that exist in X_train (and are not the target)\n",
        "columns_to_standardize = [c for c in columns_to_standardize if c in X_train.columns]\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ STEP 6: Impute missing values on TRAIN only, apply to TEST â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "imputer = SimpleImputer(strategy=\"median\")\n",
        "X_train[columns_to_standardize] = imputer.fit_transform(X_train[columns_to_standardize])\n",
        "X_test[columns_to_standardize] = imputer.transform(X_test[columns_to_standardize])\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ STEP 7: Standardize TRAIN only, apply to TEST â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "scaler = StandardScaler()\n",
        "X_train[columns_to_standardize] = scaler.fit_transform(X_train[columns_to_standardize])\n",
        "X_test[columns_to_standardize] = scaler.transform(X_test[columns_to_standardize])\n",
        "\n",
        "print(\"Columns flipped:\", len(cols_to_flip))\n",
        "print(\"Columns standardized:\", len(columns_to_standardize))\n"
      ],
      "metadata": {
        "id": "wPFCcyC9VL1k",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_test.shape\n"
      ],
      "metadata": {
        "id": "1B_lTpOuO7YM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parameters ={'C':[0.01,0.1,1],\n",
        "             'penalty':['l2'],\n",
        "             'solver':['lbfgs']}"
      ],
      "metadata": {
        "id": "r1Ju5kLKPSuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parameters ={\"C\":[0.01,0.1,1],'penalty':['l2'], 'solver':['lbfgs']}# l1 lasso l2 ridge\n",
        "lr=LogisticRegression()\n",
        "logreg_cv = GridSearchCV(estimator=lr, param_grid=parameters, cv=10)\n",
        "\n",
        "\n",
        "# Step 4: Fit the GridSearchCV object to the training data\n",
        "logreg_cv.fit(X_train, Y_train)"
      ],
      "metadata": {
        "id": "C40Ldf4iPXjX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nâ˜… Logistic Regression\")\n",
        "print(\"    best params  :\", logreg_cv.best_params_)\n",
        "print(f\"    CV accuracy  : {logreg_cv.best_score_:.3f}\")\n",
        "print(f\"    TEST accuracy: {logreg_cv.score(X_test, Y_test):.3f}\")\n",
        "\n",
        "# Predict on test set\n",
        "Y_pred = logreg_cv.predict(X_test)\n"
      ],
      "metadata": {
        "id": "AygVLpfbPaAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_lr = logreg_cv.best_estimator_\n",
        "coef = pd.Series(best_lr.coef_[0], index=X.columns, name=\"Î²\") \\\n",
        "          .sort_values(key=lambda s: s.abs(), ascending=False)\n",
        "\n",
        "display(coef.to_frame())        # full table\n",
        "\n",
        "(\n",
        "    coef.head(10)\n",
        "        .sort_values()          # order for tidy barh\n",
        "        .plot(kind=\"barh\", figsize=(7, 4))\n",
        ")\n",
        "plt.title(\"Top-10 Logistic-Regression feature weights\")\n",
        "plt.xlabel(\"Coefficient (Î²)\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CLJaVCWlREQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "best_lr = logreg_cv.best_estimator_\n",
        "coefs = best_lr.coef_.ravel()\n",
        "\n",
        "# build a scaled full X in the SAME column order as training\n",
        "X_scaled_full = pd.concat([X_train, X_test], axis=0).loc[:, X_train.columns]\n",
        "\n",
        "contributions = X_scaled_full.to_numpy() * coefs\n",
        "average_absolute_impact = np.abs(contributions).mean(axis=0)\n",
        "\n",
        "impact_series = pd.Series(average_absolute_impact, index=X_train.columns).sort_values(ascending=False)\n",
        "\n",
        "display(impact_series.head(10).to_frame(name=\"Average Impact\"))\n",
        "\n",
        "impact_series.head(10).sort_values().plot(kind=\"barh\", figsize=(7, 4))\n",
        "plt.title(\"Top 10 Feature Impacts (Average Absolute Contribution)\")\n",
        "plt.xlabel(\"Average Contribution to Prediction\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "O7hndnnUmKE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "best_lr = logreg_cv.best_estimator_\n",
        "coefs = best_lr.coef_.ravel()\n",
        "\n",
        "X_scaled_full = pd.concat([X_train, X_test], axis=0).loc[:, X_train.columns]\n",
        "\n",
        "contributions = X_scaled_full.to_numpy() * coefs\n",
        "negative_contributions = np.where(contributions < 0, contributions, 0)\n",
        "average_negative_impact = np.abs(negative_contributions).mean(axis=0)\n",
        "\n",
        "impact_series = pd.Series(average_negative_impact, index=X_train.columns).sort_values(ascending=False)\n",
        "\n",
        "print(\"\\nğŸ”» Average Negative Contribution of Each Feature (FULL):\\n\")\n",
        "display(impact_series.head(10).to_frame(name=\"Average Negative Impact\"))\n",
        "\n",
        "impact_series.head(10).sort_values().plot(kind=\"barh\", figsize=(7, 4))\n",
        "plt.title(\"Top 10 Negative Feature Impacts (Full)\")\n",
        "plt.xlabel(\"Avg. Negative Contribution to Prediction\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "h7_OgAwOQT8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "best_lr = logreg_cv.best_estimator_\n",
        "\n",
        "X_scaled_full = pd.concat([X_train, X_test], axis=0).loc[:, X_train.columns]\n",
        "\n",
        "probs = best_lr.predict_proba(X_scaled_full)[:, 1]\n",
        "p_term = (probs * (1 - probs))[:, np.newaxis]\n",
        "marginal_effects = p_term * best_lr.coef_\n",
        "average_marginal_effect = marginal_effects.mean(axis=0).ravel()\n",
        "\n",
        "ame_series = pd.Series(average_marginal_effect, index=X_train.columns).abs().sort_values(ascending=False)\n",
        "\n",
        "print(\"\\nâœ… Top 10 Features by Average Marginal Effect (AME) â€” FULL:\\n\")\n",
        "display(ame_series.head(10).to_frame(name=\"Average Marginal Effect\"))\n"
      ],
      "metadata": {
        "id": "sWLYVyByAPlV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}