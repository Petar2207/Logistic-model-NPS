{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy\n",
        "!pip install pandas\n",
        "!pip install seaborn"
      ],
      "metadata": {
        "id": "veYvgortMjKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pandas is a software library written for the Python programming language for data manipulation and analysis.\n",
        "import pandas as pd\n",
        "# NumPy is a library for the Python programming language, adding support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays\n",
        "import numpy as np\n",
        "# Matplotlib is a plotting library for python and pyplot gives us a MatLab like plotting framework. We will use this in our plotter function to plot data.\n",
        "import matplotlib.pyplot as plt\n",
        "#Seaborn is a Python data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics\n",
        "import seaborn as sns\n",
        "# Preprocessing allows us to standarsize our data\n",
        "from sklearn import preprocessing\n",
        "# Allows us to split our data into training and testing data\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Allows us to test parameters of classification algorithms and find the best one\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "# Logistic Regression classification algorithm\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "# Support Vector Machine classification algorithm\n",
        "from sklearn.svm import SVC\n",
        "# Decision Tree classification algorithm\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "# K Nearest Neighbors classification algorithm\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer"
      ],
      "metadata": {
        "id": "NdIPULlOLYjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload your CSV from local\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Load it into a DataFrame\n",
        "df = pd.read_csv(\"xy.csv\")  # replace with actual filename\n",
        "df.head()"
      ],
      "metadata": {
        "id": "hTTWK8bcNm87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question_groups_to_exclude = ['']\n",
        "manually_excluded = [col for col in df.columns if any(col == q or col.startswith(f\"{q}A\") for q in question_groups_to_exclude)]\n",
        "missing_flags = [c for c in df.columns if '_missing' in c]\n",
        "excluded_columns = set(manually_excluded + missing_flags)"
      ],
      "metadata": {
        "id": "216CEJogGvPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns_to_standardize = [\n",
        "    c for c in df.columns\n",
        "    if c not in excluded_columns and df[c].nunique() > 2\n",
        "]\n",
        "\n",
        "#Flip scale direction if needed\n",
        "always_normal_direction = ['561']  # Add known \"positive-direction\" columns here if needed\n",
        "\n",
        "df_proc = df.copy()\n",
        "cols_to_flip = []\n",
        "\n",
        "for c in columns_to_standardize:\n",
        "    if c in always_normal_direction:\n",
        "        continue\n",
        "    if df[c].nunique() == 2:\n",
        "        continue  # Skip binary columns\n",
        "    try:\n",
        "        df_proc[c] = pd.to_numeric(df_proc[c], errors='coerce')\n",
        "        col_min, col_max = df_proc[c].min(), df_proc[c].max()\n",
        "        df_proc[c] = col_max + col_min - df_proc[c]\n",
        "        cols_to_flip.append(c)\n",
        "    except:\n",
        "        print(f\"âš ï¸ Skipping flip: {c} contains non-numeric values\")\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ STEP 5: Ensure all columns are numeric â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "for col in columns_to_standardize:\n",
        "    df_proc[col] = pd.to_numeric(df_proc[col], errors='coerce')\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ STEP 6: Impute missing values â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "imputer = SimpleImputer(strategy=\"median\")\n",
        "df_proc[columns_to_standardize] = imputer.fit_transform(df_proc[columns_to_standardize])\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ STEP 7: Standardize features â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "scaler = StandardScaler()\n",
        "df_proc[columns_to_standardize] = scaler.fit_transform(df_proc[columns_to_standardize])\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ STEP 8: Prepare model inputs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "df_proc.to_csv(\"standardized_selected_columns.csv\", index=False)\n",
        "\n",
        "X = df.drop(columns=['379.2_binary'])              # original, for reference\n",
        "X_scaled = df_proc.drop(columns=['379.2_binary'])  # scaled version for modeling\n",
        "Y = df['379.2_binary'].astype(int).to_numpy()      # target column\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(\n",
        "    X_scaled, Y, test_size=0.2, random_state=2)"
      ],
      "metadata": {
        "id": "wPFCcyC9VL1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y, test_size=0.2, random_state=2)\n"
      ],
      "metadata": {
        "id": "D3z-TA5lONRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_scaled"
      ],
      "metadata": {
        "id": "PERhv0dh4gxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_test.shape\n"
      ],
      "metadata": {
        "id": "1B_lTpOuO7YM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parameters ={'C':[0.01,0.1,1],\n",
        "             'penalty':['l2'],\n",
        "             'solver':['lbfgs']}"
      ],
      "metadata": {
        "id": "r1Ju5kLKPSuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parameters ={\"C\":[0.01,0.1,1],'penalty':['l2'], 'solver':['lbfgs']}# l1 lasso l2 ridge\n",
        "lr=LogisticRegression()\n",
        "logreg_cv = GridSearchCV(estimator=lr, param_grid=parameters, cv=10)\n",
        "\n",
        "\n",
        "# Step 4: Fit the GridSearchCV object to the training data\n",
        "logreg_cv.fit(X_train, Y_train)"
      ],
      "metadata": {
        "id": "C40Ldf4iPXjX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"After read_csv     :\", df.isna().sum().sum(), \"missing\")\n",
        "print(\"After flip+cast    :\", df_proc.isna().sum().sum(), \"missing\")\n",
        "print(\"After StandardScaler:\", X_scaled.isna().sum().sum(), \"missing\")\n",
        "print(\"Infs after scaling :\", np.isinf(X_scaled).sum().sum(), \"infinite\")\n"
      ],
      "metadata": {
        "id": "RYY3aS3JLDnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nâ˜… Logistic Regression\")\n",
        "print(\"    best params  :\", logreg_cv.best_params_)\n",
        "print(f\"    CV accuracy  : {logreg_cv.best_score_:.3f}\")\n",
        "print(f\"    TEST accuracy: {logreg_cv.score(X_test, Y_test):.3f}\")\n",
        "\n",
        "# Predict on test set\n",
        "Y_pred = logreg_cv.predict(X_test)\n"
      ],
      "metadata": {
        "id": "AygVLpfbPaAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_lr = logreg_cv.best_estimator_\n",
        "coef = pd.Series(best_lr.coef_[0], index=X.columns, name=\"Î²\") \\\n",
        "          .sort_values(key=lambda s: s.abs(), ascending=False)\n",
        "\n",
        "display(coef.to_frame())        # full table\n",
        "\n",
        "(\n",
        "    coef.head(10)\n",
        "        .sort_values()          # order for tidy barh\n",
        "        .plot(kind=\"barh\", figsize=(7, 4))\n",
        ")\n",
        "plt.title(\"Top-10 Logistic-Regression feature weights\")\n",
        "plt.xlabel(\"Coefficient (Î²)\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CLJaVCWlREQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Get the best model from GridSearchCV\n",
        "best_lr = logreg_cv.best_estimator_\n",
        "\n",
        "# Get coefficients (Î² values)\n",
        "coefs = best_lr.coef_[0]  # shape: (n_features,)\n",
        "\n",
        "# Multiply each feature value by its corresponding coefficient\n",
        "# This gives the contribution of each feature per row\n",
        "contributions = X_scaled * coefs  # shape: (n_samples, n_features)\n",
        "\n",
        "# Take the mean of absolute contributions across all rows\n",
        "average_absolute_impact = np.abs(contributions).mean(axis=0)\n",
        "\n",
        "# Create a Pandas Series with feature names\n",
        "impact_series = pd.Series(average_absolute_impact, index=X.columns)\n",
        "\n",
        "# Sort by most impactful features\n",
        "impact_series = impact_series.sort_values(ascending=False)\n",
        "\n",
        "# Show results\n",
        "print(\"\\nğŸ” Top 10 Features by Average Absolute Contribution:\\n\")\n",
        "display(impact_series.nlargest(10).to_frame(name=\"Average Impact\"))\n",
        "\n",
        "\n",
        "# Optional: plot the top 10 features\n",
        "impact_series.head(10).sort_values().plot(kind=\"barh\", figsize=(7, 4))\n",
        "plt.title(\"Top 10 Feature Impacts (Average Absolute Contribution)\")\n",
        "plt.xlabel(\"Average Contribution to Prediction\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "O7hndnnUmKE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Get contributions for all rows\n",
        "contributions = X_scaled * coefs  # shape: (n_samples, n_features)\n",
        "\n",
        "# Step 2: Mask only negative contributions\n",
        "negative_contributions = np.where(contributions < 0, contributions, 0)  # keep only negatives\n",
        "\n",
        "# Step 3: Take the mean negative contribution per feature (absolute values)\n",
        "average_negative_impact = np.abs(negative_contributions).mean(axis=0)\n",
        "\n",
        "# Step 4: Create Pandas Series for labeling\n",
        "impact_series = pd.Series(average_negative_impact, index=X.columns)\n",
        "\n",
        "# Step 5: Sort descending\n",
        "impact_series = impact_series.sort_values(ascending=False)\n",
        "\n",
        "# Show result\n",
        "print(\"\\nğŸ”» Average Negative Contribution of Each Feature:\\n\")\n",
        "display(impact_series.nlargest(10).to_frame(name=\"Average Negative Impact\"))\n",
        "\n",
        "\n",
        "# Optional: Plot top 10\n",
        "impact_series.head(10).sort_values().plot(kind=\"barh\", figsize=(7, 4))\n",
        "plt.title(\"Top 10 Negative Feature Impacts\")\n",
        "plt.xlabel(\"Avg. Negative Contribution to Prediction\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "h7_OgAwOQT8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.special import expit  # this is the sigmoid function\n",
        "\n",
        "# 1. Get predicted probabilities for class 1\n",
        "probs = best_lr.predict_proba(X_scaled)[:, 1]  # shape: (n_samples,)\n",
        "\n",
        "# 2. Compute marginal effect for each feature in each row\n",
        "# Derivative of sigmoid: Î²_i * p * (1 - p)\n",
        "p_times_1_minus_p = probs * (1 - probs)  # shape: (n_samples,)\n",
        "\n",
        "# Reshape to match (n_samples, 1) for broadcasting\n",
        "p_term = p_times_1_minus_p[:, np.newaxis]  # shape: (n_samples, 1)\n",
        "\n",
        "# Multiply with coefficient vector (same as logistic marginal effect formula)\n",
        "marginal_effects = p_term * best_lr.coef_  # shape: (n_samples, n_features)\n",
        "\n",
        "# 3. Average across rows (samples) to get AME per feature\n",
        "average_marginal_effect = marginal_effects.mean(axis=0)  # shape: (n_features,)\n",
        "\n",
        "# 4. Convert to Pandas Series with feature names\n",
        "ame_series = pd.Series(average_marginal_effect, index=X.columns)\n",
        "\n",
        "# 5. Get absolute values if you care about strength, not direction\n",
        "ame_series = ame_series.abs().sort_values(ascending=False)\n",
        "\n",
        "# 6. Display top 10\n",
        "print(\"\\nâœ… Top 10 Features by Average Marginal Effect (AME):\\n\")\n",
        "display(ame_series.head(10).to_frame(name=\"Average Marginal Effect\"))\n"
      ],
      "metadata": {
        "id": "sWLYVyByAPlV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}